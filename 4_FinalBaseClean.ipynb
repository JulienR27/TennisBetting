{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8daB5Nc_KKoI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', 200, 'display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "executionInfo": {
     "elapsed": 327,
     "status": "error",
     "timestamp": 1650958716470,
     "user": {
      "displayName": "Julien Chaplet",
      "userId": "01655218241948219394"
     },
     "user_tz": -120
    },
    "id": "EgH7Mfq8MwJS",
    "outputId": "f4055fbc-69d3-4f7e-dfde-dcfa9dd2fcb5"
   },
   "outputs": [],
   "source": [
    "# we start with the merged table (ATP data + bookmakers)\n",
    "\n",
    "df = pd.read_csv('ATP_merged_clean.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L326uv1Jeu6P"
   },
   "outputs": [],
   "source": [
    "# some pre-processing on column names, this will be useful later when we switch from winner/loser to P1/P2\n",
    "\n",
    "df.rename(columns={\"Winner\": 'winner_name',\"Loser\": \"loser_name\",\"WRank\": \"w_rank\", \"LRank\": \"l_rank\", \"PSW\": \"w_PS\", \"PSL\": 'l_PS', 'B365W': 'w_B365', 'B365L': 'l_B365', 'elo_winner': 'w_elo', 'elo_loser': 'l_elo'}, errors=\"raise\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DunU1Zsyj1ph"
   },
   "outputs": [],
   "source": [
    "# get date as date format\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1dY_5_sin7ZU"
   },
   "outputs": [],
   "source": [
    "# The score column cannot be interpreted as is, therefore we made this function which parses the string to extract each game's score\n",
    "\n",
    "def score_split(score):\n",
    "  # first we take out the parenthesis from the score\n",
    "  score = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", score)\n",
    "  # we split a first time between the spaces in order to get the sets scores\n",
    "  Games = score.split()\n",
    "  G = 0\n",
    "  result = []\n",
    "  # we split a second time within each set to get each player's score\n",
    "  for game in Games:\n",
    "    scores = game.split('-')\n",
    "    for sc in scores:\n",
    "      result.append(int(sc))\n",
    "\n",
    "  # because not all games have 5 sets, we complete the missing sets with 0s\n",
    "  l = len(result)\n",
    "\n",
    "  for i in range(l, 10):\n",
    "    result.append(0)\n",
    "\n",
    "  result = tuple(result)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YdroyWD-kqB5"
   },
   "outputs": [],
   "source": [
    "# we use the function above to parse the games scores into new columns\n",
    "df['p1_1'], df['p2_1'], df['p1_2'], df['p2_2'], df['p1_3'], df['p2_3'], df['p1_4'], df['p2_4'], df['p1_5'], df['p2_5'] = zip(*map(score_split, df['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rSphDFSroVS0"
   },
   "outputs": [],
   "source": [
    "# we rename the columns to p1 and p2 instead of winner and loser\n",
    "\n",
    "pattern = '|'.join(['winner_', 'w_'])\n",
    "df.columns = df.columns.str.replace(pattern, 'p1_', regex=True)\n",
    "pattern = '|'.join(['loser_', 'l_'])\n",
    "df.columns = df.columns.str.replace(pattern, 'p2_', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hJ8VgqxPe7sX"
   },
   "outputs": [],
   "source": [
    "# we duplicate the df and reverse half in order to get a Game/Player dataset instead of a Game dataset, from this step on we get double the amount of lines\n",
    "\n",
    "to_keep = df.copy()\n",
    "to_switch = df.copy()\n",
    "\n",
    "to_keep['p1_win'] = True\n",
    "to_switch['p1_win'] = False\n",
    "\n",
    "# on the lines that we will switch, proba_elo_p2 is equal to 1 - proba_elo_p1\n",
    "to_switch['proba_elo'] = 1 - to_switch['proba_elo']\n",
    "\n",
    "# we get all the columns which are player-related and create a version with reveresed p1 and p2\n",
    "cols = list(df)\n",
    "p_cols = [x for x in cols if ('p1_' in x) or ('p2_' in x)]\n",
    "\n",
    "# there might be a better way to do this, but to revert p1 & p2 I use 3 steps and a temporary p3\n",
    "p_cols_revert = [x.replace('p1_','p3_').replace('p2_', 'p1_').replace('p3_', 'p2_') for x in p_cols]\n",
    "\n",
    "mydict = dict(zip(p_cols, p_cols_revert))\n",
    "\n",
    "# call rename () method\n",
    "to_switch.rename(columns=mydict,\n",
    "          inplace=True)\n",
    "\n",
    "# finally we group the two parts (reverted & original) of our dataframe back together\n",
    "df = to_keep.append(to_switch).sort_index().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WvzkXQ88sqTF"
   },
   "outputs": [],
   "source": [
    "# nb of sets won for each player\n",
    "\n",
    "df['p1_sets'] = (df['p1_1'] > df['p2_1']).astype(int) + (df['p1_2'] > df['p2_2']).astype(int) + (df['p1_3'] > df['p2_3']).astype(int) + (df['p1_4'] > df['p2_4']).astype(int) + (df['p1_5'] > df['p2_5']).astype(int)\n",
    "df['p2_sets'] = (df['p1_1'] < df['p2_1']).astype(int) + (df['p1_2'] < df['p2_2']).astype(int) + (df['p1_3'] < df['p2_3']).astype(int) + (df['p1_4'] < df['p2_4']).astype(int) + (df['p1_5'] < df['p2_5']).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xsG6_I0mqqIP"
   },
   "outputs": [],
   "source": [
    "# We will now try to get extra features\n",
    "\n",
    "# all these features are taken from ultimate tennis statistics, we simply reproduce the formulas with our data\n",
    "# https://www.ultimatetennisstatistics.com/glossary\n",
    "\n",
    "# 1st Serve Effectiveness\n",
    "# First Serve Effectiveness: 1st serve points won % divided by 2nd serve points won %\n",
    "df[\"p1_1stWon%\"] = df[\"p1_1stWon\"] / df[\"p1_1stIn\"]\n",
    "df[\"p1_2ndWon%\"] = df[\"p1_2ndWon\"] / (df[\"p1_svpt\"] - df[\"p1_1stIn\"])\n",
    "df[\"p1_1stServeEffectiveness\"] = df[\"p1_1stWon%\"]/df[\"p1_2ndWon%\"]\n",
    "\n",
    "df[\"p2_1stWon%\"] = df[\"p2_1stWon\"] / df[\"p2_1stIn\"]\n",
    "df[\"p2_2ndWon%\"] = df[\"p2_2ndWon\"] / (df[\"p2_svpt\"] - df[\"p2_1stIn\"])\n",
    "df[\"p2_1stServeEffectiveness\"] = df[\"p2_1stWon%\"]/df[\"p2_2ndWon%\"]\n",
    "\n",
    "# Return to Service Points Ratio \n",
    "# Return to Service Points Ratio - Return points played divided by service points played\n",
    "df[\"p1_Ret2ServPtsRatio\"] = df[\"p2_svpt\"] / df[\"p1_svpt\"]\n",
    "df[\"p2_Ret2ServPtsRatio\"] = df[\"p1_svpt\"] / df[\"p2_svpt\"]\n",
    "\n",
    "# Point Dominance Ratio\n",
    "# Points Dominance Ratio: % of return points won divided by % of service points lost\n",
    "df[\"p1_ServeWon%\"] = (df[\"p1_1stWon\"] + df[\"p1_2ndWon\"]) / df[\"p1_svpt\"]\n",
    "df[\"p1_ReturnWon%\"] = 1 - df[\"p1_ServeWon%\"]\n",
    "\n",
    "df[\"p2_ServeWon%\"] = (df[\"p2_1stWon\"] + df[\"p2_2ndWon\"]) / df[\"p2_svpt\"]\n",
    "df[\"p2_ReturnWon%\"] = 1 - df[\"p2_ServeWon%\"]\n",
    "\n",
    "df[\"p1_PtsDominanceRatio\"] = df[\"p1_ReturnWon%\"] / df[\"p2_ReturnWon%\"]\n",
    "df[\"p2_PtsDominanceRatio\"] = df[\"p2_ReturnWon%\"] / df[\"p1_ReturnWon%\"]\n",
    "\n",
    "# Break Points Ratio\n",
    "# Break Points Ratio: % of break points converted divided by % of faced break points lost\n",
    "\n",
    "df[\"p1_BPConverted%\"] = (df[\"p2_bpFaced\"] - df[\"p2_bpSaved\"]) / df[\"p2_bpFaced\"]\n",
    "df[\"p2_BPConverted%\"] = (df[\"p1_bpFaced\"] - df[\"p1_bpSaved\"]) / df[\"p1_bpFaced\"]\n",
    "\n",
    "df[\"p1_BPRatio\"] = df[\"p1_BPConverted%\"] / df[\"p2_BPConverted%\"]\n",
    "df[\"p2_BPRatio\"] = df[\"p2_BPConverted%\"] / df[\"p1_BPConverted%\"]\n",
    "\n",
    "# Points to Sets Over-Performing Ratio\n",
    "# Points to Sets Over-Performing Ratio - Points to Sets Over-Performing Ratio: % of sets won divided by % of total points won\n",
    "df[\"p1_SetWon%\"] = df[\"p1_sets\"] / (df[\"p1_sets\"] + df[\"p2_sets\"])\n",
    "df[\"p1_PtsWon%\"] = (df[\"p1_1stWon\"] + df[\"p1_2ndWon\"] + df[\"p2_1stIn\"] - df[\"p2_1stWon\"] + (df[\"p2_svpt\"] - df[\"p2_1stIn\"]) - df[\"p2_2ndWon\"]) / (df[\"p1_svpt\"] + df[\"p2_svpt\"])\n",
    "df[\"p1_Pts2Sets_OP_Ratio\"] = df[\"p1_SetWon%\"] / df[\"p1_PtsWon%\"]\n",
    "\n",
    "df[\"p2_SetWon%\"] = df[\"p2_sets\"] / (df[\"p1_sets\"] + df[\"p2_sets\"])\n",
    "df[\"p2_PtsWon%\"] = (df[\"p2_1stWon\"] + df[\"p2_2ndWon\"] + df[\"p1_1stIn\"] - df[\"p1_1stWon\"] + (df[\"p1_svpt\"] - df[\"p1_1stIn\"]) - df[\"p1_2ndWon\"]) / (df[\"p1_svpt\"] + df[\"p2_svpt\"])\n",
    "df[\"p2_Pts2Sets_OP_Ratio\"] = df[\"p2_SetWon%\"] / df[\"p2_PtsWon%\"]\n",
    "\n",
    "# Points to Games Over-Performing Ratio\n",
    "# Points to Games Over-Performing Ratio - Points to Games Over-Performing Ratio: % of games won divided by % of total points won\n",
    "df[\"p1_GmsWon%\"] = (df[\"p1_1\"] + df[\"p1_2\"] + df[\"p1_3\"] + df[\"p1_4\"] + df[\"p1_5\"]) / (df[\"p1_1\"] + df[\"p1_2\"] + df[\"p1_3\"] + df[\"p1_4\"] + df[\"p1_5\"] + df[\"p2_1\"] + df[\"p2_2\"] + df[\"p2_3\"] + df[\"p2_4\"] + df[\"p2_5\"])\n",
    "df[\"p1_Pts2Gms_OP_Ratio\"] = df[\"p1_GmsWon%\"] / df[\"p1_PtsWon%\"]\n",
    "\n",
    "df[\"p2_GmsWon%\"] = (df[\"p2_1\"] + df[\"p2_2\"] + df[\"p2_3\"] + df[\"p2_4\"] + df[\"p2_5\"]) / (df[\"p1_1\"] + df[\"p1_2\"] + df[\"p1_3\"] + df[\"p1_4\"] + df[\"p1_5\"] + df[\"p2_1\"] + df[\"p2_2\"] + df[\"p2_3\"] + df[\"p2_4\"] + df[\"p2_5\"])\n",
    "df[\"p2_Pts2Gms_OP_Ratio\"] = df[\"p2_GmsWon%\"] / df[\"p2_PtsWon%\"]\n",
    "\n",
    "# Games to Sets Over-Performing Ratio\n",
    "# Games to Sets Over-Performing Ratio - Games to Sets Over-Performing Ratio: % of sets won divided by % of games won\n",
    "df[\"p1_Gms2Sets_OP_Ratio\"] = df[\"p1_SetWon%\"] / df[\"p1_GmsWon%\"]\n",
    "df[\"p2_Gms2Sets_OP_Ratio\"] = df[\"p2_SetWon%\"] / df[\"p2_GmsWon%\"]\n",
    "\n",
    "# Break Points Over-Performing Ratio\n",
    "# Break Points Over-Performing Ratio - Break Points Over-Performing Ratio: % of break points won (saved + converted) divided by % of total points won\n",
    "df[\"p1_BPWon%\"] = (df[\"p2_bpFaced\"] - df[\"p2_bpSaved\"] + df[\"p1_bpSaved\"]) / (df[\"p1_bpFaced\"] + df[\"p2_bpFaced\"])\n",
    "df[\"p1_BP_OP_Ratio\"] = df[\"p1_BPWon%\"] / df[\"p1_PtsWon%\"]\n",
    "\n",
    "df[\"p2_BPWon%\"] = (df[\"p1_bpFaced\"] - df[\"p1_bpSaved\"] + df[\"p2_bpSaved\"]) / (df[\"p1_bpFaced\"] + df[\"p2_bpFaced\"])\n",
    "df[\"p2_BP_OP_Ratio\"] = df[\"p2_BPWon%\"] / df[\"p2_PtsWon%\"]\n",
    "\n",
    "# Break Points Saved Over-Performing Ratio\n",
    "# Break Points Saved Over-Performing Ratio - Break Points Saved Over-Performing Ratio: % of break points saved divided by % of service points won\n",
    "df[\"p1_BPSaved%\"] = df[\"p1_bpSaved\"] / df[\"p1_bpFaced\"]\n",
    "df[\"p1_BPSaved_OP_Ratio\"] = df[\"p1_BPSaved%\"] / df[\"p1_ServeWon%\"]\n",
    "\n",
    "df[\"p2_BPSaved%\"] = df[\"p2_bpSaved\"] / df[\"p2_bpFaced\"]\n",
    "df[\"p2_BPSaved_OP_Ratio\"] = df[\"p2_BPSaved%\"] / df[\"p2_ServeWon%\"]\n",
    "\n",
    "# Break Points Converted Over-Performing Ratio\n",
    "# Break Points Converted Over-Performing Ratio - Break Points Converted Over-Performing Ratio: % of break points converted divided by % of return points won\n",
    "df[\"p1_BPConverted_OP_Ratio\"] = df[\"p1_BPConverted%\"] / df[\"p1_ReturnWon%\"]\n",
    "df[\"p2_BPConverted_OP_Ratio\"] = df[\"p2_BPConverted%\"] / df[\"p2_ReturnWon%\"]\n",
    "\n",
    " # Extras\n",
    "df[\"p1_Ace%\"] = df[\"p1_ace\"]/df[\"p1_svpt\"]\n",
    "df[\"p1_DF%\"] = df[\"p1_df\"]/df[\"p1_svpt\"]\n",
    "df[\"p1_1stServe%\"] = df[\"p1_1stIn\"] / df[\"p1_svpt\"]\n",
    "df[\"p1_1stReturnWon%\"] = (df[\"p2_1stIn\"] - df[\"p2_1stWon\"]) / df[\"p2_1stIn\"]\n",
    "\n",
    "df[\"p2_Ace%\"] = df[\"p2_ace\"]/df[\"p2_svpt\"]\n",
    "df[\"p2_DF%\"] = df[\"p2_df\"]/df[\"p2_svpt\"]\n",
    "df[\"p2_1stServe%\"] = df[\"p2_1stIn\"] / df[\"p2_svpt\"]\n",
    "df[\"p2_1stReturnWon%\"] = (df[\"p1_1stIn\"] - df[\"p1_1stWon\"]) / df[\"p1_1stIn\"]\n",
    "\n",
    "# Upsets\n",
    "df[\"p1_UpsetScored\"] = [1 if (row[\"p1_rank\"] < row[\"p2_rank\"] and row[\"p1_win\"] == 1) else 0 for i,row in df.iterrows()]\n",
    "df[\"p2_UpsetScored\"] = [1 if (row[\"p1_rank\"] > row[\"p2_rank\"] and row[\"p1_win\"] == 0) else 0 for i,row in df.iterrows()]\n",
    "df[\"p1_UpsetAgainst\"] = df[\"p2_UpsetScored\"]\n",
    "df[\"p2_UpsetAgainst\"] = df[\"p1_UpsetScored\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ir7eUzIPd7m4"
   },
   "outputs": [],
   "source": [
    "# we drop the lines with NA and create a dataframe called df_ra\n",
    "\n",
    "df = df.dropna(axis=0)\n",
    "df_ra = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JnxTVFfwLpnU"
   },
   "outputs": [],
   "source": [
    "# so far our dataframe contains the in-game data for each game, of course we will not have this data available before the games\n",
    "# in order to base our model on data available at the beginning of each game (when we want to predict its outcome)\n",
    "# we will replace the in-game data with rolling averages of the previous games of each player\n",
    "\n",
    "# below we can set the parameters for this rolling aerage calculation\n",
    "min_periods = 1\n",
    "window = 60\n",
    "\n",
    "# window_short = 10\n",
    "# window_long = 30\n",
    "\n",
    "# this is the list of in-game features that we only get once the game is finished, and that we therefore need to process with rolling average\n",
    "calculated_features_p1 = ['p1_1stWon%',\n",
    "'p1_2ndWon%',\n",
    "'p1_1stServeEffectiveness',\n",
    "'p1_Ret2ServPtsRatio',\n",
    "'p1_ServeWon%',\n",
    "'p1_ReturnWon%',\n",
    "'p1_PtsDominanceRatio',\n",
    "'p1_BPConverted%',\n",
    "'p1_BPRatio',\n",
    "'p1_SetWon%',\n",
    "'p1_PtsWon%',\n",
    "'p1_Pts2Sets_OP_Ratio',\n",
    "'p1_GmsWon%',\n",
    "'p1_Pts2Gms_OP_Ratio',\n",
    "'p1_Gms2Sets_OP_Ratio',\n",
    "'p1_BPWon%',\n",
    "'p1_BP_OP_Ratio',\n",
    "'p1_BPSaved%',\n",
    "'p1_BPSaved_OP_Ratio',\n",
    "'p1_BPConverted_OP_Ratio',\n",
    "'p1_Ace%',\n",
    "'p1_DF%',\n",
    "'p1_1stServe%',\n",
    "'p1_1stReturnWon%',\n",
    "'p1_UpsetScored',\n",
    "'p1_UpsetAgainst']\n",
    "\n",
    "calculated_features_p2 = [x.replace('p1_', 'p2_') for x in calculated_features_p1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tXx-LHddg8_D"
   },
   "outputs": [],
   "source": [
    "# these dataframes are used only to experiment with different rolling average techniques\n",
    "\n",
    "df_ra2 = df_ra.copy()\n",
    "df_ra3 = df_ra.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvSAO_koJTvg"
   },
   "source": [
    "In the following part we experiment with different rolling averages techniques. In order to maintain clarity these experiments are done on copies of the dataframe, and we will only use one of these copies in the end.\n",
    "Once we are more confident that we found the best technique, we could remove the others to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AnP8M2xTeK-5"
   },
   "outputs": [],
   "source": [
    "# simple exponential\n",
    "\n",
    "# calculated_features_p2.extend(['minutes'])\n",
    "# df_ra.sort_index(ascending=False)\n",
    "\n",
    "for feature in calculated_features_p1:\n",
    "  df_ra2[feature] = df_ra2.groupby('p1_name')[feature].transform(lambda s: s[::1].shift(1).ewm(span=window).mean().round(4))\n",
    "\n",
    "for feature in calculated_features_p2:\n",
    "  df_ra2[feature] = df_ra2.groupby('p2_name')[feature].transform(lambda s: s[::1].shift(1).ewm(span=window).mean().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "k6Ayrc8apLk1"
   },
   "outputs": [],
   "source": [
    "# time related exponential\n",
    "\n",
    "half_life = 365\n",
    "\n",
    "# calculated_features_p2.extend(['minutes'])\n",
    "\n",
    "# df_ra.sort_index(ascending=False)\n",
    "\n",
    "#gpby = df_ra3.groupby('p1_name').transform(lambda s: s[::1].shift(1)).rolling(window=window, min_periods=min_periods, method=\"table\")\n",
    "\n",
    "\n",
    "#  df_ra[(feature)] = df_ra.groupby('p1_name')[feature].transform(lambda s: s[::1].shift(1).rolling(window=window, min_periods=min_periods).mean().round(4))\n",
    "\n",
    "#for feature in calculated_features_p1:\n",
    "#  df_ra2[feature] = df_ra2.groupby('p1_name')[feature].transform(lambda s: s[::1].shift(1).ewm(span=window).mean().round(4))\n",
    "\n",
    "#for feature in calculated_features_p2:\n",
    "#  df_ra2[feature] = df_ra2.groupby('p2_name')[feature].transform(lambda s: s[::1].shift(1).ewm(span=window).mean().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "W9KTDe79eG8t"
   },
   "outputs": [],
   "source": [
    "# simple rolling average\n",
    "\n",
    "for feature in calculated_features_p1:\n",
    "  df_ra[(feature)] = df_ra.groupby('p1_name')[feature].transform(lambda s: s[::1].shift(1).rolling(window=window, min_periods=min_periods).mean())\n",
    "\n",
    "for feature in calculated_features_p2:\n",
    "  df_ra[(feature)] = df_ra.groupby('p2_name')[feature].transform(lambda s: s[::1].shift(1).rolling(window=window, min_periods=min_periods).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "NWWQc0fflVUr"
   },
   "outputs": [],
   "source": [
    "\n",
    "# finally for some features we thought that it could be interesting to get each player's 'trend': to know if they are in a positive or negative progression over their last games\n",
    "# to do so, we compare the position at the time of the game with the average of the positions over a set amount of previous games.\n",
    "# we do this with everything which concerns ranking. This data is available pre-game\n",
    "\n",
    "# window_trend\n",
    "\n",
    "# df_ra['p1_rank_change'] = (df_ra['p1_rank'] - df_ra.groupby('p1_name')['p1_rank'].transform(lambda s: s[::1].shift(1).rolling(window=window, min_periods=min_periods).mean())) / df_ra['p1_rank']\n",
    "# df_ra['p2_rank_change'] = (df_ra['p2_rank'] - df_ra.groupby('p2_name')['p2_rank'].transform(lambda s: s[::1].shift(1).rolling(window=window, min_periods=min_periods).mean())) / df_ra['p2_rank']\n",
    "\n",
    "# df_ra['p1_rank_points_change'] = (df_ra['p1_rank_points'] - df_ra.groupby('p1_name')['p1_rank_points'].transform(lambda s: s[::1].shift(1).rolling(window=window, min_periods=min_periods).mean())) / df_ra['p1_rank_points']\n",
    "# df_ra['p2_rank_points_change'] = (df_ra['p2_rank_points'] - df_ra.groupby('p2_name')['p2_rank_points'].transform(lambda s: s[::1].shift(1).rolling(window=window, min_periods=min_periods).mean())) / df_ra['p2_rank_points']\n",
    "\n",
    "# df_ra['p1_elo_change'] = (df_ra['p1_elo'] - df_ra.groupby('p1_name')['p1_elo'].transform(lambda s: s[::1].shift(1).rolling(window=window, min_periods=min_periods).mean())) / df_ra['p1_elo']\n",
    "# df_ra['p2_elo_change'] = (df_ra['p2_elo'] - df_ra.groupby('p2_name')['p2_elo'].transform(lambda s: s[::1].shift(1).rolling(window=window, min_periods=min_periods).mean())) / df_ra['p2_elo']\n",
    "\n",
    "#ATP and elo ranks change with relative change\n",
    "df_ra['p1_rank_change'] = df_ra.groupby('p1_name')['p1_rank'].transform(lambda r: r.shift(1).rolling(window=window, min_periods=2).apply(lambda x: (x.iloc[-1]-x.dropna().iloc[0])/x.dropna().iloc[0]))\n",
    "df_ra['p2_rank_change'] = df_ra.groupby('p2_name')['p2_rank'].transform(lambda r: r.shift(1).rolling(window=window, min_periods=2).apply(lambda x: (x.iloc[-1]-x.dropna().iloc[0])/x.dropna().iloc[0]))\n",
    "\n",
    "df_ra['p1_rank_points_change'] = df_ra.groupby('p1_name')['p1_rank_points'].transform(lambda r: r.shift(1).rolling(window=window, min_periods=2).apply(lambda x: (x.iloc[-1]-x.dropna().iloc[0])/x.dropna().iloc[0]))\n",
    "df_ra['p2_rank_points_change'] = df_ra.groupby('p2_name')['p2_rank_points'].transform(lambda r: r.shift(1).rolling(window=window, min_periods=2).apply(lambda x: (x.iloc[-1]-x.dropna().iloc[0])/x.dropna().iloc[0]))\n",
    "\n",
    "df_ra['p1_elo_change'] = df_ra.groupby('p1_name')['p1_elo'].transform(lambda r: r.shift(1).rolling(window=window, min_periods=2).apply(lambda x: (x.iloc[-1]-x.dropna().iloc[0])/x.dropna().iloc[0]))\n",
    "df_ra['p2_elo_change'] = df_ra.groupby('p2_name')['p2_elo'].transform(lambda r: r.shift(1).rolling(window=window, min_periods=2).apply(lambda x: (x.iloc[-1]-x.dropna().iloc[0])/x.dropna().iloc[0]))\n",
    "\n",
    "# #ATP and elo ranks change with moving average crossing\n",
    "# df_ra['p1_rank_change'] = df_ra.groupby('p1_name')['p1_rank'].transform(lambda r: (r.shift(1).rolling(window=window_short, min_periods=2).mean() / r.shift(1).rolling(window=window_long, min_periods=2).mean()) - 1)\n",
    "# df_ra['p2_rank_change'] = df_ra.groupby('p2_name')['p2_rank'].transform(lambda r: (r.shift(1).rolling(window=window_short, min_periods=2).mean() / r.shift(1).rolling(window=window_long, min_periods=2).mean()) - 1)\n",
    "\n",
    "# df_ra['p1_rank_points_change'] = df_ra.groupby('p1_name')['p1_rank_points'].transform(lambda r: (r.shift(1).rolling(window=window_short, min_periods=2).mean() / r.shift(1).rolling(window=window_long, min_periods=2).mean()) - 1)\n",
    "# df_ra['p2_rank_points_change'] = df_ra.groupby('p2_name')['p2_rank_points'].transform(lambda r: (r.shift(1).rolling(window=window_short, min_periods=2).mean() / r.shift(1).rolling(window=window_long, min_periods=2).mean()) -1) \n",
    "\n",
    "# df_ra['p1_elo_change'] = df_ra.groupby('p1_name')['p1_elo'].transform(lambda r: (r.shift(1).rolling(window=window_short, min_periods=2).mean() / r.shift(1).rolling(window=window_long, min_periods=2).mean()) - 1)\n",
    "# df_ra['p2_elo_change'] = df_ra.groupby('p2_name')['p2_elo'].transform(lambda r: (r.shift(1).rolling(window=window_short, min_periods=2).mean() / r.shift(1).rolling(window=window_long, min_periods=2).mean()) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put NA to 0 because they correspond to the first appearance of the players in the database, therefore they have a 'progression' at 0.\n",
    "df_ra = df_ra.fillna({'p1_rank_change':0, 'p2_rank_change':0, 'p1_rank_points_change':0, 'p2_rank_points_change':0, 'p1_elo_change':0, 'p2_elo_change':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "g0dFQVNTf4_E"
   },
   "outputs": [],
   "source": [
    "# when trying to predict the outcome of the game, the 'strength' of each player is not enough,\n",
    "# we neeed our model to be able to process the features of each player in relation to the other's\n",
    "# we will change all the features to get ratios P1 / P2 instead of seperate columns\n",
    "\n",
    "features = ['1stWon%',\n",
    "'2ndWon%',\n",
    "'1stServeEffectiveness',\n",
    "'Ret2ServPtsRatio',\n",
    "'ServeWon%',\n",
    "'ReturnWon%',\n",
    "'PtsDominanceRatio',\n",
    "'BPConverted%',\n",
    "'BPRatio',\n",
    "'SetWon%',\n",
    "'PtsWon%',\n",
    "'Pts2Sets_OP_Ratio',\n",
    "'GmsWon%',\n",
    "'Pts2Gms_OP_Ratio',\n",
    "'Gms2Sets_OP_Ratio',\n",
    "'BPWon%',\n",
    "'BP_OP_Ratio',\n",
    "'BPSaved%',\n",
    "'BPSaved_OP_Ratio',\n",
    "'BPConverted_OP_Ratio',\n",
    "'Ace%',\n",
    "'DF%',\n",
    "'1stServe%',\n",
    "'1stReturnWon%',\n",
    "'rank_points',\n",
    "'rank',\n",
    "'age',\n",
    "'ht',\n",
    "'elo']\n",
    "\n",
    "# we put the ratios in place of the p1 columns and then drop p2 to keep only the ratios\n",
    "\n",
    "for feature in features:\n",
    "  #df_ra[('Diff_' + feature)] = df_ra[('p2_' + feature)] - df_ra[('p1_' + feature)]\n",
    "  df_ra[('p1_' + feature)] = df_ra[('p1_' + feature)] / df_ra[('p2_' + feature)]\n",
    "  df_ra.rename({('p1_' + feature):('ratio_' + feature)}, axis=1, inplace=True)\n",
    "  df_ra.drop(('p2_' + feature), axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmFHCHdSLBJe"
   },
   "outputs": [],
   "source": [
    "# finally, we save the file for future pre modeling processing and modeling\n",
    "# at this point the dataframe can still use a dropna and drop inf\n",
    "\n",
    "df_ra.to_csv('DTB_Rolling_Features_ratios_w60.csv', sep=',')\n",
    "# df.to_csv('/content/drive/MyDrive/ProjetSports/DataTennis/DTB_Features.csv', sep=',')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02_Merged_preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
